# Automated destruction of ArgoCD Agent infrastructure
# Separate workflows for Hub and Spoke clusters with safety confirmations
# - Terraform-managed resource destruction
# - PKI certificate cleanup
# - Namespace and resource removal
name: Destroy ArgoCD Infrastructure

on:
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type "DESTROY" to confirm (case-sensitive)'
        required: true
        type: string
      destroy_hub:
        description: "Destroy hub cluster"
        required: false
        type: boolean
        default: false
      destroy_spokes:
        description: "Destroy spoke clusters"
        required: false
        type: boolean
        default: true

  # Push trigger for testing on feature branch 
  # push:
  #   branches:
  #     - "pipeline/Argocd"
  #   paths:
  #     - ".github/workflows/destroy-argocd-infrastructure.yaml"

# Required permissions
permissions:
  contents: read
  id-token: write

# Global environment variables
env:
  TERRAFORM_VERSION: "1.9.0"
  KUBECTL_VERSION: "1.28.0"
  WORKING_DIR: "argocd-agent/terraform/environments/prod"
  CLOUD_PROVIDER: "gke"
  HUB_NAMESPACE: ${{ vars.HUB_NAMESPACE || 'argocd' }}
  SPOKE_NAMESPACE: ${{ vars.SPOKE_NAMESPACE || 'argocd' }}

jobs:
  # Validation job: Determine what to destroy
  validate-destroy:
    name: Validate Destroy Request
    runs-on: ubuntu-latest
    outputs:
      destroy_hub: ${{ steps.validate.outputs.destroy_hub }}
      destroy_spokes: ${{ steps.validate.outputs.destroy_spokes }}

    steps:
      - name: Validate confirmation
        id: validate
        run: |
          if [ "${{ github.event_name }}" = "push" ]; then
            echo "âš¡ Push trigger on branch â€” using input defaults for destroy toggles"
            # On push: use the destroy_hub/destroy_spokes input defaults (both true)
            # Since inputs are not available on push, we must hardcode the defaults or use logic
            DESTROY_HUB="true"
            DESTROY_SPOKES="true"
          else
            # On workflow_dispatch: require DESTROY confirmation
            CONFIRM="${{ inputs.confirm_destroy }}"
            if [ "$CONFIRM" != "DESTROY" ]; then
              echo "âŒ ERROR: Confirmation failed!"
              echo "   You must type 'DESTROY' (case-sensitive) to proceed"
              exit 1
            fi
            echo "âœ… Destroy confirmation validated"

            # Use toggle inputs directly
            DESTROY_HUB="${{ inputs.destroy_hub }}"
            DESTROY_SPOKES="${{ inputs.destroy_spokes }}"
          fi

          echo "destroy_hub=$DESTROY_HUB" >> $GITHUB_OUTPUT
          echo "destroy_spokes=$DESTROY_SPOKES" >> $GITHUB_OUTPUT

          echo "Destroy Hub: $DESTROY_HUB"
          echo "Destroy Spokes: $DESTROY_SPOKES"

  # Destroy spoke clusters first (must be destroyed before hub for clean PKI removal)
  destroy-spokes:
    name: Destroy Spoke Clusters
    runs-on: ubuntu-latest
    needs: [validate-destroy]
    if: needs.validate-destroy.outputs.destroy_spokes == 'true'
    environment:
      name: production-destroy

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Netbird CLI
        run: |
          curl -fsSL https://pkgs.netbird.io/install.sh | sh

      - name: Connect to Netbird
        run: |
          netbird up --setup-key ${{ secrets.NETBIRD_SETUP_KEY_RUNNERS }}

      - name: Wait for Netbird peer sync
        run: |
          echo "â³ Waiting for Netbird peer sync..."
          MAX_WAIT=60
          WAITED=0

          while [ $WAITED -lt $MAX_WAIT ]; do
            PEERS=$(netbird status | grep "Peers count" | awk '{print $3}')
            echo "Current peers: $PEERS (waited ${WAITED}s)"

            if echo "$PEERS" | grep -q "/.*Connected" && ! echo "$PEERS" | grep -q "0/0"; then
              echo "âœ… Peers synced!"
              netbird status
              exit 0
            fi

            sleep 5
            WAITED=$((WAITED + 5))
          done

          echo "âš  Peer sync timeout after ${MAX_WAIT}s, proceeding anyway..."
          netbird status

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup GCP credentials
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Install GKE auth plugin
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

          # Configure hub cluster (needed for PKI operations)
          gcloud container clusters get-credentials ${{ secrets.HUB_CLUSTER_NAME }} \
            --region=${{ secrets.HUB_CLUSTER_LOCATION }} \
            --project=${{ secrets.GCP_PROJECT_ID }}

      # Generate kubeconfig for spoke clusters (needed for resource cleanup)
      - name: Configure spoke cluster access
        run: |
          echo "Configuring kubectl for spoke clusters..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "âš  SPOKE_CLUSTERS secret not set, using default: spoke-2"
            SPOKE_CLUSTERS="spoke-2"
          fi

          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"

          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            echo "Configuring $cluster..."

            # Get Netbird IP and certificates based on cluster name
            case "$cluster" in
              spoke-2)
                # Helper: User wants to use spoke-2 context but with spoke-1 credentials
                NETBIRD_IP="${{ secrets.SPOKE_1_NETBIRD_IP }}"
                NETBIRD_IP="${NETBIRD_IP%/32}"
                CA_CERT="${{ secrets.SPOKE_1_CA_CERT }}"
                CLIENT_CERT="${{ secrets.SPOKE_1_CLIENT_CERT }}"
                CLIENT_KEY="${{ secrets.SPOKE_1_CLIENT_KEY }}"
                ;;
              *)
                echo "  âš  Unknown cluster: $cluster, skipping"
                continue
                ;;
            esac

            if [ ! -z "$NETBIRD_IP" ]; then
              kubectl config set-cluster $cluster \
                --server=https://$NETBIRD_IP:6443 \
                --certificate-authority=<(echo "$CA_CERT" | base64 -d) \
                --embed-certs=true

              kubectl config set-credentials $cluster-admin \
                --client-certificate=<(echo "$CLIENT_CERT" | base64 -d) \
                --client-key=<(echo "$CLIENT_KEY" | base64 -d) \
                --embed-certs=true

              kubectl config set-context $cluster \
                --cluster=$cluster \
                --user=$cluster-admin
            fi
          done

          kubectl config get-contexts

      # Validate that all clusters in SPOKE_CLUSTERS have their contexts configured
      - name: Validate spoke cluster contexts
        run: |
          echo "Validating that all spoke clusters have kubectl contexts configured..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "âš  SPOKE_CLUSTERS not configured, skipping validation"
            exit 0
          fi

          MISSING_CONTEXTS=""
          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            if ! kubectl config get-contexts -o name | grep -q "^${cluster}$"; then
              MISSING_CONTEXTS="$MISSING_CONTEXTS\n  - $cluster"
            fi
          done

          if [ ! -z "$MISSING_CONTEXTS" ]; then
            echo ""
            echo "âŒ ERROR: The following clusters are in SPOKE_CLUSTERS but their kubectl contexts were not created:"
            echo -e "$MISSING_CONTEXTS"
            echo ""
            echo "This usually means the required secrets are missing. For each spoke cluster, you need:"
            echo "  - SPOKE_N_NETBIRD_IP"
            echo "  - SPOKE_N_CA_CERT"
            echo "  - SPOKE_N_CLIENT_CERT"
            echo "  - SPOKE_N_CLIENT_KEY"
            echo ""
            echo "Either:"
            echo "  1. Add the missing secrets to GitHub for these clusters, OR"
            echo "  2. Remove these clusters from the SPOKE_CLUSTERS secret"
            exit 1
          fi

          echo "âœ… All spoke clusters have valid contexts configured"

      - name: Cleanup old backend configs
        working-directory: ${{ env.WORKING_DIR }}
        run: rm -f backend.tf backend-config.tf

      - name: Configure backend
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          bash ../../../../.github/scripts/configure-backend.sh "${{ env.CLOUD_PROVIDER }}" "argocd-spokes"
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}

      - name: Terraform Init
        working-directory: ${{ env.WORKING_DIR }}
        run: terraform init

      # Backup state before destruction
      - name: Backup Terraform state
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          terraform state pull > spoke-state-backup-$(date +%Y%m%d-%H%M%S).json
          echo "State backup created"

      - name: Upload state backup
        uses: actions/upload-artifact@v6
        with:
          name: spoke-state-backup
          path: ${{ env.WORKING_DIR }}/spoke-state-backup-*.json
          retention-days: 90

      # Create destroy plan
      - name: Terraform destroy plan
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          # Recreate terraform.tfvars for destroy
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            SPOKE_CLUSTERS="spoke-1,spoke-2,spoke-3"
          fi

          CLUSTERS_MAP="{"
          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for i in "${!CLUSTERS[@]}"; do
            cluster=$(echo "${CLUSTERS[$i]}" | xargs)
            # Derive agent namespace name from cluster name.
            # Example: spoke-2 -> agent-2, spoke2 -> agent-2
            agent="$cluster"
            agent=$(echo "$agent" | sed -E 's/^spoke-?/agent-/')
            agent=$(echo "$agent" | sed -E 's/agent-+$/agent/')
            if [ $i -gt 0 ]; then
              CLUSTERS_MAP="$CLUSTERS_MAP, "
            fi
            CLUSTERS_MAP="$CLUSTERS_MAP\"$agent\" = \"$cluster\""
          done
          CLUSTERS_MAP="$CLUSTERS_MAP}"

          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ secrets.HUB_CLUSTER_LOCATION }}_${{ secrets.HUB_CLUSTER_NAME }}"

          # Dynamically resolve principal address from hub cluster
          PRINCIPAL_IP=$(kubectl get svc argocd-agent-principal \
            -n ${{ env.HUB_NAMESPACE }} \
            --context "$HUB_CONTEXT" \
            -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
          if [ -z "$PRINCIPAL_IP" ]; then
            PRINCIPAL_IP=$(kubectl get svc argocd-agent-principal \
              -n ${{ env.HUB_NAMESPACE }} \
              --context "$HUB_CONTEXT" \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
          fi
          PRINCIPAL_PORT=$(kubectl get svc argocd-agent-principal \
            -n ${{ env.HUB_NAMESPACE }} \
            --context "$HUB_CONTEXT" \
            -o jsonpath='{.spec.ports[0].port}' 2>/dev/null || echo "443")
          echo "Principal address: ${PRINCIPAL_IP:-unknown}:$PRINCIPAL_PORT"

          cat > terraform.tfvars <<EOF
          deploy_hub            = false
          deploy_spokes         = true
          hub_cluster_context   = "$HUB_CONTEXT"
          workload_clusters     = $CLUSTERS_MAP
          argocd_version        = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace         = "${{ env.HUB_NAMESPACE }}"
          spoke_namespace       = "${{ env.SPOKE_NAMESPACE }}"
          principal_address     = "${PRINCIPAL_IP:-}"
          principal_port        = ${PRINCIPAL_PORT:-443}
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          create_default_admin_user    = ${{ vars.CREATE_DEFAULT_ADMIN_USER || 'true' }}
          default_admin_username       = "${{ vars.DEFAULT_ADMIN_USERNAME || 'argocd-admin' }}"
          default_admin_email          = "${{ vars.DEFAULT_ADMIN_EMAIL || 'admin@argocd.local' }}"
          default_admin_password       = "${{ secrets.DEFAULT_ADMIN_PASSWORD }}"
          default_admin_password_temporary = ${{ vars.DEFAULT_ADMIN_PASSWORD_TEMPORARY || 'true' }}
          enable_appproject_sync = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server = "*"
          appproject_default_dest_namespaces = ["*"]
          EOF

          terraform plan -destroy -out=destroy-plan

      # Execute destroy
      - name: Terraform destroy spokes
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  Destroying spoke cluster infrastructure..."
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          terraform apply -auto-approve destroy-plan
          echo "âœ… Spoke clusters destroyed"

      # Manual cleanup of any remaining resources
      - name: Cleanup remaining resources
        run: |
          echo "Checking for remaining spoke cluster resources..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "No spoke clusters configured"
            exit 0
          fi

          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            echo "Checking $cluster..."

            if kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster 2>/dev/null; then
              echo "âœ“ Found namespace ${{ env.SPOKE_NAMESPACE }} on $cluster, attempting clean deletion..."
              
              # Check for failing APIServices that block discovery and namespace termination
              FAILING_SERVICES=$(kubectl get apiservice --context $cluster | grep "False" | awk '{print $1}' || true)
              if [ ! -z "$FAILING_SERVICES" ]; then
                echo "âš  Found failing APIServices that may block namespace deletion:"
                echo "$FAILING_SERVICES"
                for svc in $FAILING_SERVICES; do
                  echo "Deleting failing APIService: $svc"
                  kubectl delete apiservice $svc --context $cluster --ignore-not-found=true --timeout=30s || true
                done
              fi

              # Normal delete first
              kubectl delete namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster --ignore-not-found=true --timeout=60s || true
              
              # If still exists, use the finalize API method
              if kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster 2>/dev/null; then
                echo "âš  Namespace still exists, attempting finalizer removal via API..."
                kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster -o json | \
                  sed 's/"finalizers": \[[^]]*\]/"finalizers": []/' > tmp_ns.json
                kubectl replace --raw "/api/v1/namespaces/${{ env.SPOKE_NAMESPACE }}/finalize" -f tmp_ns.json --context $cluster || true
                rm tmp_ns.json
              fi
              
              echo "âœ“ Cleanup attempt completed for $cluster"
            else
              echo "âœ“ Namespace ${{ env.SPOKE_NAMESPACE }} already removed from $cluster"
            fi
          done

      # Trigger deploy workflow after successful destroy (only on push events AND if hub was preserved)
      - name: Trigger deploy spokes workflow
        # Only run if push event AND we did NOT destroy the hub
        if: github.event_name == 'push' && needs.validate-destroy.outputs.destroy_hub == 'false'
        run: |
          echo "ğŸ”„ Triggering deploy workflow after successful spoke destruction..."

          # Use GitHub CLI to trigger the deploy workflow
          # We use terraform_action=apply to actually deploy the spokes
          gh workflow run deploy-argocd-spokes-netbird.yaml \
            --ref ${{ github.ref_name }} \
            -f terraform_action=apply

          echo "âœ… Deploy workflow triggered successfully"
          echo "   Check the Actions tab to monitor the deployment"
        env:
          GH_TOKEN: ${{ github.token }}

  # Destroy hub cluster
  destroy-hub:
    name: Destroy Hub Cluster
    runs-on: ubuntu-latest
    needs: [validate-destroy, destroy-spokes]
    if: |
      always() && 
      needs.validate-destroy.outputs.destroy_hub == 'true' &&
      (needs.destroy-spokes.result == 'success' || needs.destroy-spokes.result == 'skipped')
    environment:
      name: production-destroy

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup GCP credentials
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3

      - name: Detect cluster location
        id: cluster_location
        run: |
          CLUSTER_INFO=$(gcloud container clusters list --project=${{ secrets.GCP_PROJECT_ID }} --format="value(name,location)" | grep "^${{ secrets.HUB_CLUSTER_NAME }}" || true)
          if [ -z "$CLUSTER_INFO" ]; then
            echo "ERROR: Cluster '${{ secrets.HUB_CLUSTER_NAME }}' not found!"
            gcloud container clusters list --project=${{ secrets.GCP_PROJECT_ID }} --format="table(name,location)"
            exit 1
          fi
          ACTUAL_LOCATION=$(echo "$CLUSTER_INFO" | awk '{print $2}')
          echo "âœ“ Found cluster at location: $ACTUAL_LOCATION"
          if [[ "$ACTUAL_LOCATION" =~ -[a-z]$ ]]; then
            LOCATION_FLAG="--zone"
          else
            LOCATION_FLAG="--region"
          fi
          echo "actual_location=$ACTUAL_LOCATION" >> $GITHUB_OUTPUT
          echo "location_flag=$LOCATION_FLAG" >> $GITHUB_OUTPUT

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

          gcloud container clusters get-credentials ${{ secrets.HUB_CLUSTER_NAME }} \
            ${{ steps.cluster_location.outputs.location_flag }}=${{ steps.cluster_location.outputs.actual_location }} \
            --project=${{ secrets.GCP_PROJECT_ID }}

      # CRITICAL: Backup PKI before destroying
      - name: Backup PKI CA certificate
        run: |
          echo "ğŸ” Backing up PKI CA certificate..."
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          if kubectl get secret argocd-agent-pki-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            kubectl get secret argocd-agent-pki-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o yaml > pki-ca-final-backup-$(date +%Y%m%d-%H%M%S).yaml
            echo "âœ… PKI CA backed up (argocd-agent-pki-ca)"
          elif kubectl get secret argocd-agent-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            kubectl get secret argocd-agent-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o yaml > pki-ca-final-backup-$(date +%Y%m%d-%H%M%S).yaml
            echo "âœ… PKI CA backed up (argocd-agent-ca)"
          else
            echo "âš  PKI CA secret not found (may have been deleted already)"
          fi

      - name: Upload PKI backup
        uses: actions/upload-artifact@v6
        with:
          name: pki-ca-final-backup
          path: pki-ca-final-backup-*.yaml
          retention-days: 365

      - name: Cleanup old backend configs
        working-directory: ${{ env.WORKING_DIR }}
        run: rm -f backend.tf backend-config.tf

      - name: Configure backend
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          bash ../../../../.github/scripts/configure-backend.sh "${{ env.CLOUD_PROVIDER }}" "argocd-hub"
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}

      - name: Terraform Init
        working-directory: ${{ env.WORKING_DIR }}
        run: terraform init

      - name: Create terraform.tfvars
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub                   = true
          deploy_spokes                = false
          hub_cluster_context          = "$HUB_CONTEXT"
          argocd_version               = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace                = "${{ env.HUB_NAMESPACE }}"
          ui_expose_method             = "${{ vars.UI_EXPOSE_METHOD || 'ingress' }}"
          principal_expose_method      = "${{ vars.PRINCIPAL_EXPOSE_METHOD || 'loadbalancer' }}"
          argocd_host                  = "${{ secrets.ARGOCD_HOST }}"
          install_cert_manager         = ${{ vars.INSTALL_CERT_MANAGER || 'false' }}
          install_nginx_ingress        = ${{ vars.INSTALL_NGINX_INGRESS || 'false' }}
          cert_manager_version         = "${{ vars.CERT_MANAGER_VERSION || 'v1.16.2' }}"
          nginx_ingress_version        = "${{ vars.NGINX_INGRESS_VERSION || '4.11.3' }}"
          letsencrypt_email            = "${{ secrets.LETSENCRYPT_EMAIL }}"
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          create_default_admin_user    = ${{ vars.CREATE_DEFAULT_ADMIN_USER || 'true' }}
          default_admin_username       = "${{ vars.DEFAULT_ADMIN_USERNAME || 'argocd-admin' }}"
          default_admin_email          = "${{ vars.DEFAULT_ADMIN_EMAIL || 'admin@argocd.local' }}"
          default_admin_password       = "${{ secrets.DEFAULT_ADMIN_PASSWORD }}"
          default_admin_password_temporary = ${{ vars.DEFAULT_ADMIN_PASSWORD_TEMPORARY || 'true' }}
          enable_appproject_sync       = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server       = "*"
          appproject_default_dest_namespaces   = ["*"]
          EOF

      # Backup state before destruction
      - name: Backup Terraform state
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          terraform state pull > hub-state-backup-$(date +%Y%m%d-%H%M%S).json
          echo "State backup created"

      - name: Upload state backup
        uses: actions/upload-artifact@v6
        with:
          name: hub-state-backup
          path: ${{ env.WORKING_DIR }}/hub-state-backup-*.json
          retention-days: 90

      # Create destroy plan
      - name: Terraform destroy plan
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub                   = true
          deploy_spokes                = false
          hub_cluster_context          = "$HUB_CONTEXT"
          argocd_version               = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace                = "${{ env.HUB_NAMESPACE }}"
          ui_expose_method             = "${{ vars.UI_EXPOSE_METHOD || 'ingress' }}"
          principal_expose_method      = "${{ vars.PRINCIPAL_EXPOSE_METHOD || 'loadbalancer' }}"
          argocd_host                  = "${{ secrets.ARGOCD_HOST }}"
          install_cert_manager         = ${{ vars.INSTALL_CERT_MANAGER || 'false' }}
          install_nginx_ingress        = ${{ vars.INSTALL_NGINX_INGRESS || 'false' }}
          cert_manager_version         = "${{ vars.CERT_MANAGER_VERSION || 'v1.16.2' }}"
          nginx_ingress_version        = "${{ vars.NGINX_INGRESS_VERSION || '4.11.3' }}"
          letsencrypt_email            = "${{ secrets.LETSENCRYPT_EMAIL }}"
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          create_default_admin_user    = ${{ vars.CREATE_DEFAULT_ADMIN_USER || 'true' }}
          default_admin_username       = "${{ vars.DEFAULT_ADMIN_USERNAME || 'argocd-admin' }}"
          default_admin_email          = "${{ vars.DEFAULT_ADMIN_EMAIL || 'admin@argocd.local' }}"
          default_admin_password       = "${{ secrets.DEFAULT_ADMIN_PASSWORD }}"
          default_admin_password_temporary = ${{ vars.DEFAULT_ADMIN_PASSWORD_TEMPORARY || 'true' }}
          enable_appproject_sync       = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server = "*"
          appproject_default_dest_namespaces = ["*"]
          EOF

          terraform plan -destroy -out=destroy-plan

      # Execute destroy
      - name: Terraform destroy hub
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  Destroying hub cluster infrastructure..."
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          terraform apply -auto-approve destroy-plan
          echo "âœ… Hub cluster destroyed"

      # Manual cleanup of any remaining resources
      - name: Cleanup remaining resources
        run: |
          echo "Checking for remaining hub cluster resources..."
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          if kubectl get namespace ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            echo "âœ“ Found namespace ${{ env.HUB_NAMESPACE }} on hub cluster, attempting clean deletion..."
            
            # Check for failing APIServices that block discovery and namespace termination
            FAILING_SERVICES=$(kubectl get apiservice --context $HUB_CONTEXT | grep "False" | awk '{print $1}' || true)
            if [ ! -z "$FAILING_SERVICES" ]; then
              echo "âš  Found failing APIServices that may block namespace deletion:"
              echo "$FAILING_SERVICES"
              for svc in $FAILING_SERVICES; do
                echo "Deleting failing APIService: $svc"
                kubectl delete apiservice $svc --context $HUB_CONTEXT --ignore-not-found=true --timeout=30s || true
              done
            fi

            # Normal delete first
            kubectl delete namespace ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT --ignore-not-found=true --timeout=60s || true
            
            # If still exists, use the finalize API method
            if kubectl get namespace ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
              echo "âš  Namespace still exists, attempting finalizer removal via API..."
              kubectl get namespace ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o json | \
                sed 's/"finalizers": \[[^]]*\]/"finalizers": []/' > tmp_ns_hub.json
              kubectl replace --raw "/api/v1/namespaces/${{ env.HUB_NAMESPACE }}/finalize" -f tmp_ns_hub.json --context $HUB_CONTEXT || true
              rm tmp_ns_hub.json
            fi
            
            echo "âœ“ Cleanup attempt completed for hub cluster"
          else
             echo "âœ“ Namespace ${{ env.HUB_NAMESPACE }} already removed from hub cluster"
          fi

      - name: Destroy summary
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "âœ… Hub infrastructure destruction complete"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ“‹ Backups created:"
          echo "   - PKI CA certificate (retained for 365 days)"
          echo "   - Terraform state (retained for 90 days)"
          echo ""
          echo "âš   IMPORTANT:"
          echo "   - Verify all resources have been removed from GKE console"
          echo "   - Check for any orphaned LoadBalancers or persistent volumes"
          echo "   - Terraform state files remain in GCS bucket: ${{ secrets.TF_STATE_BUCKET }}"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
