# Automated destruction of ArgoCD Agent infrastructure
# Separate workflows for Hub and Spoke clusters with safety confirmations
# - Terraform-managed resource destruction
# - PKI certificate cleanup
# - Namespace and resource removal
name: Destroy ArgoCD Infrastructure

on:
  workflow_dispatch:
    inputs:
      target:
        description: 'What to destroy (hub, spokes, all)'
        required: false
        default: 'hub'
        type: choice
        options:
          - hub
          - spokes
          - all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm (case-sensitive)'
        required: true
        type: string
      skip_confirmation:
        description: 'Skip manual approval (use with caution!)'
        required: false
        type: boolean
        default: false

  push:
    paths:
      - 'argocd-agent/terraform/**'
      - '.github/workflows/destroy-argocd-infrastructure.yaml'

# Required permissions
permissions:
  contents: read
  id-token: write

# Global environment variables 
env:
  TERRAFORM_VERSION: '1.9.0'
  KUBECTL_VERSION: '1.28.0'
  WORKING_DIR: 'argocd-agent/terraform/environments/prod'
  CLOUD_PROVIDER: 'gke'
  HUB_NAMESPACE: ${{ vars.HUB_NAMESPACE || 'argocd' }}
  SPOKE_NAMESPACE: ${{ vars.SPOKE_NAMESPACE || 'argocd' }}

jobs:
  # Validation job: Ensure user confirmation
  validate-destroy:
    name: Validate Destroy Request
    runs-on: ubuntu-latest
    outputs:
      destroy_hub: ${{ steps.validate.outputs.destroy_hub }}
      destroy_spokes: ${{ steps.validate.outputs.destroy_spokes }}

    steps:
      - name: Validate confirmation
        id: validate
        run: |
          if [ "${{ github.event_name }}" = "push" ]; then
            echo "âš  Push trigger: skipping DESTROY confirmation validation"
          else
            if [ "${{ inputs.confirm_destroy }}" != "DESTROY" ]; then
              echo "âŒ ERROR: Confirmation failed!"
              echo "   You must type 'DESTROY' (case-sensitive) to proceed"
              exit 1
            fi
          fi

          echo "âœ… Destroy confirmation validated"

          # Set outputs based on target
          TARGET="${{ inputs.target }}"
          if [ "${{ github.event_name }}" = "push" ]; then
            # On a push event, we don't want to destroy anything automatically.
            # By setting TARGET to empty, the subsequent checks will not match
            # 'hub', 'spokes', or 'all', so no destroy jobs will be triggered.
            TARGET="hub"
          fi

          if [ "$TARGET" == "hub" ] || [ "$TARGET" == "all" ]; then
            echo "destroy_hub=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_hub=false" >> $GITHUB_OUTPUT
          fi

          if [ "$TARGET" == "spokes" ] || [ "$TARGET" == "all" ]; then
            echo "destroy_spokes=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_spokes=false" >> $GITHUB_OUTPUT
          fi

          echo "Target: $TARGET"
          echo "Destroy Hub: $([ "$TARGET" == "hub" ] || [ "$TARGET" == "all" ] && echo 'YES' || echo 'NO')"
          echo "Destroy Spokes: $([ "$TARGET" == "spokes" ] || [ "$TARGET" == "all" ] && echo 'YES' || echo 'NO')"


  # Destroy spoke clusters first (must be destroyed before hub for clean PKI removal)
  destroy-spokes:
    name: Destroy Spoke Clusters
    runs-on: ubuntu-latest
    needs: [validate-destroy]
    if: needs.validate-destroy.outputs.destroy_spokes == 'true'
    environment:
      name: production-destroy

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Netbird CLI
        run: |
          curl -fsSL https://pkgs.netbird.io/install.sh | sh

      - name: Connect to Netbird
        run: |
          netbird up --setup-key ${{ secrets.NETBIRD_SETUP_KEY_RUNNERS }}

      - name: Wait for Netbird peer sync
        run: |
          echo "â³ Waiting for Netbird peer sync..."
          MAX_WAIT=60
          WAITED=0

          while [ $WAITED -lt $MAX_WAIT ]; do
            PEERS=$(netbird status | grep "Peers count" | awk '{print $3}')
            echo "Current peers: $PEERS (waited ${WAITED}s)"

            if echo "$PEERS" | grep -q "/.*Connected" && ! echo "$PEERS" | grep -q "0/0"; then
              echo "âœ… Peers synced!"
              netbird status
              exit 0
            fi

            sleep 5
            WAITED=$((WAITED + 5))
          done

          echo "âš  Peer sync timeout after ${MAX_WAIT}s, proceeding anyway..."
          netbird status

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup GCP credentials
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Install GKE auth plugin
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

          # Configure hub cluster (needed for PKI operations)
          gcloud container clusters get-credentials ${{ secrets.HUB_CLUSTER_NAME }} \
            --region=${{ secrets.HUB_CLUSTER_LOCATION }} \
            --project=${{ secrets.GCP_PROJECT_ID }}

      # Generate kubeconfig for spoke clusters (needed for resource cleanup)
      - name: Configure spoke cluster access
        run: |
          echo "Configuring kubectl for spoke clusters..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "âš  SPOKE_CLUSTERS not configured, skipping kubeconfig setup"
            exit 0
          fi

          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"

          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            echo "Configuring $cluster..."

            # Get Netbird IP and certificates based on cluster name
            case "$cluster" in
              spoke-1|spoke-2)
                NETBIRD_IP="${{ secrets.SPOKE_1_NETBIRD_IP }}"
                NETBIRD_IP="${NETBIRD_IP%/32}"
                CA_CERT="${{ secrets.SPOKE_1_CA_CERT }}"
                CLIENT_CERT="${{ secrets.SPOKE_1_CLIENT_CERT }}"
                CLIENT_KEY="${{ secrets.SPOKE_1_CLIENT_KEY }}"
                ;;
              *)
                echo "  âš  Unknown cluster: $cluster, skipping"
                continue
                ;;
            esac

            if [ ! -z "$NETBIRD_IP" ]; then
              kubectl config set-cluster $cluster \
                --server=https://$NETBIRD_IP:6443 \
                --certificate-authority=<(echo "$CA_CERT" | base64 -d) \
                --embed-certs=true

              kubectl config set-credentials $cluster-admin \
                --client-certificate=<(echo "$CLIENT_CERT" | base64 -d) \
                --client-key=<(echo "$CLIENT_KEY" | base64 -d) \
                --embed-certs=true

              kubectl config set-context $cluster \
                --cluster=$cluster \
                --user=$cluster-admin
            fi
          done

          kubectl config get-contexts

      - name: Cleanup old backend configs
        working-directory: ${{ env.WORKING_DIR }}
        run: rm -f backend.tf backend-config.tf

      - name: Configure backend
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          bash ../../../../.github/scripts/configure-backend.sh "${{ env.CLOUD_PROVIDER }}" "argocd-spokes"
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}

      - name: Terraform Init
        working-directory: ${{ env.WORKING_DIR }}
        run: terraform init

      # Backup state before destruction
      - name: Backup Terraform state
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          terraform state pull > spoke-state-backup-$(date +%Y%m%d-%H%M%S).json
          echo "State backup created"

      - name: Upload state backup
        uses: actions/upload-artifact@v6
        with:
          name: spoke-state-backup
          path: ${{ env.WORKING_DIR }}/spoke-state-backup-*.json
          retention-days: 90

      # Create destroy plan
      - name: Terraform destroy plan
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          # Recreate terraform.tfvars for destroy
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            SPOKE_CLUSTERS="spoke-1,spoke-2,spoke-3"
          fi

          CLUSTERS_MAP="{"
          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for i in "${!CLUSTERS[@]}"; do
            cluster=$(echo "${CLUSTERS[$i]}" | xargs)
            # Derive agent namespace name from cluster name.
            # Example: spoke-2 -> agent-2, spoke2 -> agent-2
            agent="$cluster"
            agent=$(echo "$agent" | sed -E 's/^spoke-?/agent-/')
            agent=$(echo "$agent" | sed -E 's/agent-+$/agent/')
            if [ $i -gt 0 ]; then
              CLUSTERS_MAP="$CLUSTERS_MAP, "
            fi
            CLUSTERS_MAP="$CLUSTERS_MAP\"$agent\" = \"$cluster\""
          done
          CLUSTERS_MAP="$CLUSTERS_MAP}"

          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ secrets.HUB_CLUSTER_LOCATION }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub            = false
          deploy_spokes         = true
          hub_cluster_context   = "$HUB_CONTEXT"
          workload_clusters     = $CLUSTERS_MAP
          argocd_version        = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace         = "${{ env.HUB_NAMESPACE }}"
          spoke_namespace       = "${{ env.SPOKE_NAMESPACE }}"
          principal_address     = "${{ secrets.HUB_PRINCIPAL_ADDRESS }}"
          principal_port        = ${{ secrets.HUB_PRINCIPAL_PORT || 443 }}
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          create_default_admin_user    = ${{ vars.CREATE_DEFAULT_ADMIN_USER || 'true' }}
          default_admin_username       = "${{ vars.DEFAULT_ADMIN_USERNAME || 'argocd-admin' }}"
          default_admin_email          = "${{ vars.DEFAULT_ADMIN_EMAIL || 'admin@argocd.local' }}"
          default_admin_password       = "${{ secrets.DEFAULT_ADMIN_PASSWORD }}"
          default_admin_password_temporary = ${{ vars.DEFAULT_ADMIN_PASSWORD_TEMPORARY || 'true' }}
          enable_appproject_sync = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server = "*"
          appproject_default_dest_namespaces = ["*"]
          EOF

          terraform plan -destroy -out=destroy-plan

      # Execute destroy
      - name: Terraform destroy spokes
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  Destroying spoke cluster infrastructure..."
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          terraform apply -auto-approve destroy-plan
          echo "âœ… Spoke clusters destroyed"

      # Manual cleanup of any remaining resources
      - name: Cleanup remaining resources
        run: |
          echo "Checking for remaining spoke cluster resources..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "No spoke clusters configured"
            exit 0
          fi

          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            echo "Checking $cluster..."

            if kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster 2>/dev/null; then
              echo "âœ“ Found namespace ${{ env.SPOKE_NAMESPACE }} on $cluster, attempting clean deletion..."
              
              # Check for failing APIServices that block discovery and namespace termination
              FAILING_SERVICES=$(kubectl get apiservice --context $cluster | grep "False" | awk '{print $1}' || true)
              if [ ! -z "$FAILING_SERVICES" ]; then
                echo "âš  Found failing APIServices that may block namespace deletion:"
                echo "$FAILING_SERVICES"
                for svc in $FAILING_SERVICES; do
                  echo "Deleting failing APIService: $svc"
                  kubectl delete apiservice $svc --context $cluster --ignore-not-found=true --timeout=30s || true
                done
              fi

              # Normal delete first
              kubectl delete namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster --ignore-not-found=true --timeout=60s || true
              
              # If still exists, use the finalize API method
              if kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster 2>/dev/null; then
                echo "âš  Namespace still exists, attempting finalizer removal via API..."
                kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster -o json | \
                  sed 's/"finalizers": \[[^]]*\]/"finalizers": []/' > tmp_ns.json
                kubectl replace --raw "/api/v1/namespaces/${{ env.SPOKE_NAMESPACE }}/finalize" -f tmp_ns.json --context $cluster || true
                rm tmp_ns.json
              fi
              
              echo "âœ“ Cleanup attempt completed for $cluster"
            else
              echo "âœ“ Namespace ${{ env.SPOKE_NAMESPACE }} already removed from $cluster"
            fi
          done


  # Destroy hub cluster
  destroy-hub:
    name: Destroy Hub Cluster
    runs-on: ubuntu-latest
    needs: [validate-destroy, destroy-spokes]
    if: |
      always() && 
      needs.validate-destroy.outputs.destroy_hub == 'true' &&
      (needs.destroy-spokes.result == 'success' || needs.destroy-spokes.result == 'skipped')
    environment:
      name: production-destroy

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup GCP credentials
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3

      - name: Detect cluster location
        id: cluster_location
        run: |
          CLUSTER_INFO=$(gcloud container clusters list --project=${{ secrets.GCP_PROJECT_ID }} --format="value(name,location)" | grep "^${{ secrets.HUB_CLUSTER_NAME }}" || true)
          if [ -z "$CLUSTER_INFO" ]; then
            echo "ERROR: Cluster '${{ secrets.HUB_CLUSTER_NAME }}' not found!"
            gcloud container clusters list --project=${{ secrets.GCP_PROJECT_ID }} --format="table(name,location)"
            exit 1
          fi
          ACTUAL_LOCATION=$(echo "$CLUSTER_INFO" | awk '{print $2}')
          echo "âœ“ Found cluster at location: $ACTUAL_LOCATION"
          if [[ "$ACTUAL_LOCATION" =~ -[a-z]$ ]]; then
            LOCATION_FLAG="--zone"
          else
            LOCATION_FLAG="--region"
          fi
          echo "actual_location=$ACTUAL_LOCATION" >> $GITHUB_OUTPUT
          echo "location_flag=$LOCATION_FLAG" >> $GITHUB_OUTPUT

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

          gcloud container clusters get-credentials ${{ secrets.HUB_CLUSTER_NAME }} \
            ${{ steps.cluster_location.outputs.location_flag }}=${{ steps.cluster_location.outputs.actual_location }} \
            --project=${{ secrets.GCP_PROJECT_ID }}

      # CRITICAL: Backup PKI before destroying
      - name: Backup PKI CA certificate
        run: |
          echo "ğŸ” Backing up PKI CA certificate..."
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          if kubectl get secret argocd-agent-pki-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            kubectl get secret argocd-agent-pki-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o yaml > pki-ca-final-backup-$(date +%Y%m%d-%H%M%S).yaml
            echo "âœ… PKI CA backed up (argocd-agent-pki-ca)"
          elif kubectl get secret argocd-agent-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            kubectl get secret argocd-agent-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o yaml > pki-ca-final-backup-$(date +%Y%m%d-%H%M%S).yaml
            echo "âœ… PKI CA backed up (argocd-agent-ca)"
          else
            echo "âš  PKI CA secret not found (may have been deleted already)"
          fi

      - name: Upload PKI backup
        uses: actions/upload-artifact@v6
        with:
          name: pki-ca-final-backup
          path: pki-ca-final-backup-*.yaml
          retention-days: 365

      - name: Cleanup old backend configs
        working-directory: ${{ env.WORKING_DIR }}
        run: rm -f backend.tf backend-config.tf

      - name: Configure backend
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          bash ../../../../.github/scripts/configure-backend.sh "${{ env.CLOUD_PROVIDER }}" "argocd-hub"
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}

      - name: Terraform Init
        working-directory: ${{ env.WORKING_DIR }}
        run: terraform init

      - name: Create terraform.tfvars
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub                   = true
          deploy_spokes                = false
          hub_cluster_context          = "$HUB_CONTEXT"
          argocd_version               = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace                = "${{ env.HUB_NAMESPACE }}"
          ui_expose_method             = "${{ vars.UI_EXPOSE_METHOD || 'ingress' }}"
          principal_expose_method      = "${{ vars.PRINCIPAL_EXPOSE_METHOD || 'loadbalancer' }}"
          argocd_host                  = "${{ secrets.ARGOCD_HOST }}"
          install_cert_manager         = ${{ vars.INSTALL_CERT_MANAGER || 'false' }}
          install_nginx_ingress        = ${{ vars.INSTALL_NGINX_INGRESS || 'false' }}
          cert_manager_version         = "${{ vars.CERT_MANAGER_VERSION || 'v1.16.2' }}"
          nginx_ingress_version        = "${{ vars.NGINX_INGRESS_VERSION || '4.11.3' }}"
          letsencrypt_email            = "${{ secrets.LETSENCRYPT_EMAIL }}"
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          create_default_admin_user    = ${{ vars.CREATE_DEFAULT_ADMIN_USER || 'true' }}
          default_admin_username       = "${{ vars.DEFAULT_ADMIN_USERNAME || 'argocd-admin' }}"
          default_admin_email          = "${{ vars.DEFAULT_ADMIN_EMAIL || 'admin@argocd.local' }}"
          default_admin_password       = "${{ secrets.DEFAULT_ADMIN_PASSWORD }}"
          default_admin_password_temporary = ${{ vars.DEFAULT_ADMIN_PASSWORD_TEMPORARY || 'true' }}
          enable_appproject_sync       = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server       = "*"
          appproject_default_dest_namespaces   = ["*"]
          EOF

      # Backup state before destruction
      - name: Backup Terraform state
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          terraform state pull > hub-state-backup-$(date +%Y%m%d-%H%M%S).json
          echo "State backup created"

      - name: Upload state backup
        uses: actions/upload-artifact@v6
        with:
          name: hub-state-backup
          path: ${{ env.WORKING_DIR }}/hub-state-backup-*.json
          retention-days: 90

      # Create destroy plan
      - name: Terraform destroy plan
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub                   = true
          deploy_spokes                = false
          hub_cluster_context          = "$HUB_CONTEXT"
          argocd_version               = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace                = "${{ env.HUB_NAMESPACE }}"
          ui_expose_method             = "${{ vars.UI_EXPOSE_METHOD || 'ingress' }}"
          principal_expose_method      = "${{ vars.PRINCIPAL_EXPOSE_METHOD || 'loadbalancer' }}"
          argocd_host                  = "${{ secrets.ARGOCD_HOST }}"
          install_cert_manager         = ${{ vars.INSTALL_CERT_MANAGER || 'false' }}
          install_nginx_ingress        = ${{ vars.INSTALL_NGINX_INGRESS || 'false' }}
          cert_manager_version         = "${{ vars.CERT_MANAGER_VERSION || 'v1.16.2' }}"
          nginx_ingress_version        = "${{ vars.NGINX_INGRESS_VERSION || '4.11.3' }}"
          letsencrypt_email            = "${{ secrets.LETSENCRYPT_EMAIL }}"
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          enable_appproject_sync       = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server = "*"
          appproject_default_dest_namespaces = ["*"]
          EOF

          terraform plan -destroy -out=destroy-plan

      # Execute destroy
      - name: Terraform destroy hub
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  Destroying hub cluster infrastructure..."
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          terraform apply -auto-approve destroy-plan
          echo "âœ… Hub cluster destroyed"

      # Manual cleanup of any remaining resources
      - name: Cleanup remaining resources
        run: |
          echo "Checking for remaining hub cluster resources..."
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          if kubectl get namespace ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            echo "âš  Namespace ${{ env.HUB_NAMESPACE }} still exists on hub cluster"
            echo "   Manual cleanup may be required"
          fi

      - name: Destroy summary
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "âœ… Hub infrastructure destruction complete"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ“‹ Backups created:"
          echo "   - PKI CA certificate (retained for 365 days)"
          echo "   - Terraform state (retained for 90 days)"
          echo ""
          echo "âš   IMPORTANT:"
          echo "   - Verify all resources have been removed from GKE console"
          echo "   - Check for any orphaned LoadBalancers or persistent volumes"
          echo "   - Terraform state files remain in GCS bucket: ${{ secrets.TF_STATE_BUCKET }}"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"