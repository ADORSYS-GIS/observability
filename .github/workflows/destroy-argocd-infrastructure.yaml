# Automated destruction of ArgoCD Agent infrastructure
# Separate workflows for Hub and Spoke clusters with safety confirmations
# - Terraform-managed resource destruction
# - PKI certificate cleanup
# - Namespace and resource removal
name: Destroy ArgoCD Infrastructure

on:
  workflow_dispatch:
    inputs:
      target:
        description: 'Target Infrastructure'
        required: true
        type: choice
        options:
          - hub
          - spokes
          - all
        default: 'spokes'
      confirm_destroy:
        description: 'Type "DESTROY" to confirm (case-sensitive)'
        required: true
        type: string
      skip_confirmation:
        description: 'Skip manual approval (use with caution!)'
        required: false
        type: boolean
        default: false

  push:
    paths:
      - 'argocd-agent/terraform/**'
      - '.github/workflows/destroy-argocd-infrastructure.yaml'

# Required permissions
permissions:
  contents: read
  id-token: write

# Global environment variables
env:
  TERRAFORM_VERSION: '1.9.0'
  KUBECTL_VERSION: '1.28.0'
  WORKING_DIR: 'argocd-agent/terraform/environments/prod'
  CLOUD_PROVIDER: 'gke'
  HUB_NAMESPACE: ${{ vars.HUB_NAMESPACE || 'argocd' }}
  SPOKE_NAMESPACE: ${{ vars.SPOKE_NAMESPACE || 'argocd' }}

jobs:
  # Validation job: Ensure user confirmation
  validate-destroy:
    name: Validate Destroy Request
    runs-on: ubuntu-latest
    outputs:
      destroy_hub: ${{ steps.validate.outputs.destroy_hub }}
      destroy_spokes: ${{ steps.validate.outputs.destroy_spokes }}

    steps:
      - name: Validate confirmation
        id: validate
        run: |
          if [ "${{ github.event_name }}" = "push" ]; then
            echo "âš  Push trigger: skipping DESTROY confirmation validation"
          else
            if [ "${{ inputs.confirm_destroy }}" != "DESTROY" ]; then
              echo "âŒ ERROR: Confirmation failed!"
              echo "   You must type 'DESTROY' (case-sensitive) to proceed"
              exit 1
            fi
          fi

          echo "âœ… Destroy confirmation validated"

          # Set outputs based on target
          TARGET="${{ inputs.target }}"
          if [ "${{ github.event_name }}" = "push" ]; then
            TARGET="hub"
          fi

          if [ "$TARGET" == "hub" ] || [ "$TARGET" == "all" ]; then
            echo "destroy_hub=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_hub=false" >> $GITHUB_OUTPUT
          fi

          if [ "$TARGET" == "spokes" ] || [ "$TARGET" == "all" ]; then
            echo "destroy_spokes=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_spokes=false" >> $GITHUB_OUTPUT
          fi

          echo "Target: $TARGET"
          echo "Destroy Hub: $([ "$TARGET" == "hub" ] || [ "$TARGET" == "all" ] && echo 'YES' || echo 'NO')"
          echo "Destroy Spokes: $([ "$TARGET" == "spokes" ] || [ "$TARGET" == "all" ] && echo 'YES' || echo 'NO')"


  # Destroy spoke clusters first (must be destroyed before hub for clean PKI removal)
  destroy-spokes:
    name: Destroy Spoke Clusters
    runs-on: ubuntu-latest
    needs: [validate-destroy]
    if: needs.validate-destroy.outputs.destroy_spokes == 'true'
    environment:
      name: production-destroy

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Connect to Netbird to access spoke clusters
      - name: Connect to Netbird
        uses: Alemiz112/netbird-connect@v1
        with:
          setup_key: ${{ secrets.NETBIRD_SETUP_KEY_RUNNERS }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup GCP credentials
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Configure hub cluster (needed for PKI operations)
          gcloud container clusters get-credentials ${{ secrets.HUB_CLUSTER_NAME }} \
            --region=${{ secrets.HUB_CLUSTER_LOCATION }} \
            --project=${{ secrets.GCP_PROJECT_ID }}

      # Generate kubeconfig for spoke clusters (needed for resource cleanup)
      - name: Configure spoke cluster access
        run: |
          echo "Configuring kubectl for spoke clusters..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "âš  SPOKE_CLUSTERS not configured, skipping kubeconfig setup"
            exit 0
          fi

          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            echo "Configuring $cluster..."
            # Simplified - actual implementation would use secrets as in deploy workflow
          done

      - name: Cleanup old backend configs
        working-directory: ${{ env.WORKING_DIR }}
        run: rm -f backend.tf backend-config.tf

      - name: Configure backend
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          bash ../../.github/scripts/configure-backend.sh "${{ env.CLOUD_PROVIDER }}" "argocd-spokes"
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}

      - name: Terraform Init
        working-directory: ${{ env.WORKING_DIR }}
        run: terraform init

      # Backup state before destruction
      - name: Backup Terraform state
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          terraform state pull > spoke-state-backup-$(date +%Y%m%d-%H%M%S).json
          echo "State backup created"

      - name: Upload state backup
        uses: actions/upload-artifact@v6
        with:
          name: spoke-state-backup
          path: ${{ env.WORKING_DIR }}/spoke-state-backup-*.json
          retention-days: 90

      # Create destroy plan
      - name: Terraform destroy plan
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          # Recreate terraform.tfvars for destroy
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            SPOKE_CLUSTERS="spoke-1,spoke-2,spoke-3"
          fi

          CLUSTERS_MAP="{"
          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for i in "${!CLUSTERS[@]}"; do
            cluster=$(echo "${CLUSTERS[$i]}" | xargs)
            if [ $i -gt 0 ]; then
              CLUSTERS_MAP="$CLUSTERS_MAP, "
            fi
            CLUSTERS_MAP="$CLUSTERS_MAP\"$cluster\" = \"$cluster\""
          done
          CLUSTERS_MAP="$CLUSTERS_MAP}"

          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ secrets.HUB_CLUSTER_LOCATION }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub            = false
          deploy_spokes         = true
          hub_cluster_context   = "$HUB_CONTEXT"
          workload_clusters     = $CLUSTERS_MAP
          argocd_version        = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace         = "${{ env.HUB_NAMESPACE }}"
          spoke_namespace       = "${{ env.SPOKE_NAMESPACE }}"
          principal_address     = "${{ secrets.HUB_PRINCIPAL_ADDRESS }}"
          principal_port        = ${{ secrets.HUB_PRINCIPAL_PORT || 443 }}
          enable_appproject_sync = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server = "*"
          appproject_default_dest_namespaces = ["*"]
          EOF

          terraform plan -destroy -out=destroy-plan

      # Execute destroy
      - name: Terraform destroy spokes
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  Destroying spoke cluster infrastructure..."
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          terraform apply -auto-approve destroy-plan
          echo "âœ… Spoke clusters destroyed"

      # Manual cleanup of any remaining resources
      - name: Cleanup remaining resources
        run: |
          echo "Checking for remaining spoke cluster resources..."
          SPOKE_CLUSTERS="${{ secrets.SPOKE_CLUSTERS }}"
          if [ -z "$SPOKE_CLUSTERS" ]; then
            echo "No spoke clusters configured"
            exit 0
          fi

          IFS=',' read -ra CLUSTERS <<< "$SPOKE_CLUSTERS"
          for cluster in "${CLUSTERS[@]}"; do
            cluster=$(echo "$cluster" | xargs)
            echo "Checking $cluster..."

            if kubectl get namespace ${{ env.SPOKE_NAMESPACE }} --context $cluster 2>/dev/null; then
              echo "âš  Namespace ${{ env.SPOKE_NAMESPACE }} still exists on $cluster"
              echo "   Manual cleanup may be required"
            fi
          done


  # Destroy hub cluster (should be done AFTER spokes for clean PKI cleanup)
  destroy-hub:
    name: Destroy Hub Cluster
    runs-on: ubuntu-latest
    needs: [validate-destroy, destroy-spokes]
    if: |
      always() &&
      needs.validate-destroy.outputs.destroy_hub == 'true' &&
      (needs.destroy-spokes.result == 'success' || needs.destroy-spokes.result == 'skipped')
    environment:
      name: production-destroy

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup GCP credentials
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3

      - name: Detect cluster location
        id: cluster_location
        run: |
          CLUSTER_INFO=$(gcloud container clusters list --project=${{ secrets.GCP_PROJECT_ID }} --format="value(name,location)" | grep "^${{ secrets.HUB_CLUSTER_NAME }}" || true)
          if [ -z "$CLUSTER_INFO" ]; then
            echo "ERROR: Cluster '${{ secrets.HUB_CLUSTER_NAME }}' not found!"
            gcloud container clusters list --project=${{ secrets.GCP_PROJECT_ID }} --format="table(name,location)"
            exit 1
          fi
          ACTUAL_LOCATION=$(echo "$CLUSTER_INFO" | awk '{print $2}')
          echo "âœ“ Found cluster at location: $ACTUAL_LOCATION"
          if [[ "$ACTUAL_LOCATION" =~ -[a-z]$ ]]; then
            LOCATION_FLAG="--zone"
          else
            LOCATION_FLAG="--region"
          fi
          echo "actual_location=$ACTUAL_LOCATION" >> $GITHUB_OUTPUT
          echo "location_flag=$LOCATION_FLAG" >> $GITHUB_OUTPUT

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

          gcloud container clusters get-credentials ${{ secrets.HUB_CLUSTER_NAME }} \
            ${{ steps.cluster_location.outputs.location_flag }}=${{ steps.cluster_location.outputs.actual_location }} \
            --project=${{ secrets.GCP_PROJECT_ID }}

      # CRITICAL: Backup PKI before destroying
      - name: Backup PKI CA certificate
        run: |
          echo "ğŸ” Backing up PKI CA certificate..."
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          if kubectl get secret argocd-agent-pki-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            kubectl get secret argocd-agent-pki-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o yaml > pki-ca-final-backup-$(date +%Y%m%d-%H%M%S).yaml
            echo "âœ… PKI CA backed up (argocd-agent-pki-ca)"
          elif kubectl get secret argocd-agent-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            kubectl get secret argocd-agent-ca -n ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT -o yaml > pki-ca-final-backup-$(date +%Y%m%d-%H%M%S).yaml
            echo "âœ… PKI CA backed up (argocd-agent-ca)"
          else
            echo "âš  PKI CA secret not found (may have been deleted already)"
          fi

      - name: Upload PKI backup
        uses: actions/upload-artifact@v6
        with:
          name: pki-ca-final-backup
          path: pki-ca-final-backup-*.yaml
          retention-days: 365

      - name: Cleanup old backend configs
        working-directory: ${{ env.WORKING_DIR }}
        run: rm -f backend.tf backend-config.tf

      - name: Configure backend
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          bash ../../../../.github/scripts/configure-backend.sh "${{ env.CLOUD_PROVIDER }}" "argocd-hub"
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}

      - name: Terraform Init
        working-directory: ${{ env.WORKING_DIR }}
        run: terraform init

      # Backup state before destruction
      - name: Backup Terraform state
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          terraform state pull > hub-state-backup-$(date +%Y%m%d-%H%M%S).json
          echo "State backup created"

      - name: Upload state backup
        uses: actions/upload-artifact@v6
        with:
          name: hub-state-backup
          path: ${{ env.WORKING_DIR }}/hub-state-backup-*.json
          retention-days: 90

      # Create destroy plan
      - name: Terraform destroy plan
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          cat > terraform.tfvars <<EOF
          deploy_hub                   = true
          deploy_spokes                = false
          hub_cluster_context          = "$HUB_CONTEXT"
          argocd_version               = "${{ vars.ARGOCD_VERSION || 'v0.5.3' }}"
          hub_namespace                = "${{ env.HUB_NAMESPACE }}"
          ui_expose_method             = "${{ vars.UI_EXPOSE_METHOD || 'ingress' }}"
          principal_expose_method      = "${{ vars.PRINCIPAL_EXPOSE_METHOD || 'loadbalancer' }}"
          argocd_host                  = "${{ secrets.ARGOCD_HOST }}"
          install_cert_manager         = ${{ vars.INSTALL_CERT_MANAGER || 'false' }}
          install_nginx_ingress        = ${{ vars.INSTALL_NGINX_INGRESS || 'false' }}
          cert_manager_version         = "${{ vars.CERT_MANAGER_VERSION || 'v1.16.2' }}"
          nginx_ingress_version        = "${{ vars.NGINX_INGRESS_VERSION || '4.11.3' }}"
          letsencrypt_email            = "${{ secrets.LETSENCRYPT_EMAIL }}"
          enable_keycloak              = ${{ vars.ENABLE_KEYCLOAK || 'false' }}
          keycloak_url                 = "${{ secrets.KEYCLOAK_URL }}"
          keycloak_user                = "${{ secrets.KEYCLOAK_USER }}"
          keycloak_password            = "${{ secrets.KEYCLOAK_PASSWORD }}"
          keycloak_realm               = "${{ vars.KEYCLOAK_REALM || 'argocd' }}"
          argocd_url                   = "${{ secrets.ARGOCD_URL }}"
          enable_appproject_sync       = true
          appproject_default_source_namespaces = ["*"]
          appproject_default_dest_server = "*"
          appproject_default_dest_namespaces = ["*"]
          EOF

          terraform plan -destroy -out=destroy-plan

      # Execute destroy
      - name: Terraform destroy hub
        working-directory: ${{ env.WORKING_DIR }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  Destroying hub cluster infrastructure..."
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          terraform apply -auto-approve destroy-plan
          echo "âœ… Hub cluster destroyed"

      # Manual cleanup of any remaining resources
      - name: Cleanup remaining resources
        run: |
          echo "Checking for remaining hub cluster resources..."
          HUB_CONTEXT="gke_${{ secrets.GCP_PROJECT_ID }}_${{ steps.cluster_location.outputs.actual_location }}_${{ secrets.HUB_CLUSTER_NAME }}"

          if kubectl get namespace ${{ env.HUB_NAMESPACE }} --context $HUB_CONTEXT 2>/dev/null; then
            echo "âš  Namespace ${{ env.HUB_NAMESPACE }} still exists on hub cluster"
            echo "   Manual cleanup may be required"
          fi

      - name: Destroy summary
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "âœ… Hub infrastructure destruction complete"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ“‹ Backups created:"
          echo "   - PKI CA certificate (retained for 365 days)"
          echo "   - Terraform state (retained for 90 days)"
          echo ""
          echo "âš   IMPORTANT:"
          echo "   - Verify all resources have been removed from GKE console"
          echo "   - Check for any orphaned LoadBalancers or persistent volumes"
          echo "   - Terraform state files remain in GCS bucket: ${{ secrets.TF_STATE_BUCKET }}"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
